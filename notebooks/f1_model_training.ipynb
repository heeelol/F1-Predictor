{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is dedicated to developing a robust machine learning model capable of accurately predicting Formula 1 race ranks, providing valuable insights for F1 enthusiasts and strategic analysis.\n",
    "\n",
    "Building upon the comprehensive data exploration and feature engineering from `f1_data_exploration.ipynb`, we consolidate relevant race and qualifying data into a unified pandas DataFrame. This dataset is then meticulously cleaned and prepared, leveraging a diverse set of features encompassing driver and constructor historical performance, qualifying results, circuit characteristics, and more, to capture the complex dynamics of F1 racing.\n",
    "\n",
    "The core of this notebook focuses on identifying the most suitable machine learning model and its respective hyperparameters. We will systematically explore `RandomForestRegressor` and `GradientBoostingRegressor` using advanced cross-validation and hyperparameter tuning techniques from `sklearn.model_selection`. The optimization process will prioritize not just the accuracy of predicted F1 race results, but also the efficiency and generalization of the training procedure. Model performance will be rigorously evaluated using metrics such as Mean Absolute Error (MAE) and R-squared, with a particular focus on how well the model predicts the actual finishing order.\n",
    "\n",
    "Finally, the best-performing model, along with its optimized parameters, will be serialized and saved as `f1_rank_predictor.pkl` in the `models/` directory, ready for seamless integration into the backend prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import fastf1\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Race Data\n",
    "\n",
    "Race data for years 2015 to 2025 are loaded into `notebooks\\data_cache`.\n",
    "\n",
    "__NOTE__: Even with `time.sleep()` calls to avoid exceeding the API call limit, we face errors in retrieving data, both into `notebooks/data_cache` and the respective `full_race_results_df` and `full_quali_results_df` dataframes. To avoid such issues, please skip to the \"Load data from csv\" section after running the above imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configure FastF1 Caching ---\n",
    "cache_dir = './data_cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    # Added print for absolute path to help user verify cache location\n",
    "    print(f\"Absolute path of cache directory: {os.path.abspath(cache_dir)}\")\n",
    "    print(f\"Created cache directory: {cache_dir}\")\n",
    "else:\n",
    "    print(f\"Cache directory already exists: {cache_dir}\")\n",
    "    # Added print for absolute path even if directory exists\n",
    "    print(f\"Absolute path of cache directory: {os.path.abspath(cache_dir)}\")\n",
    "\n",
    "\n",
    "fastf1.Cache.enable_cache(cache_dir)\n",
    "print(f\"FastF1 caching enabled to '{cache_dir}'\")\n",
    "\n",
    "# Explicitly set log level to INFO to see caching details\n",
    "fastf1.set_log_level('INFO')\n",
    "print(\"\\nFastF1 log level set to INFO to show detailed caching messages.\")\n",
    "\n",
    "\n",
    "# --- 2. Define the Years You Want to Read ---\n",
    "start_year = 2015\n",
    "end_year = 2025 # Includes the current year, 2025\n",
    "\n",
    "# Lists to hold DataFrames from each race\n",
    "all_race_results = []\n",
    "all_qualifying_results = []\n",
    "\n",
    "# Dictionaries to track loading status and errors\n",
    "year_loading_status = {}\n",
    "\n",
    "print(f\"\\n--- Starting data collection for seasons {start_year} to {end_year} ---\")\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    year_loading_status[year] = {'status': 'Pending', 'races_loaded': 0, 'qual_loaded': 0, 'errors': []}\n",
    "    \n",
    "    try:\n",
    "        # Give a small pause before fetching schedule for a new year\n",
    "        time.sleep(1) # Pause for 1 second\n",
    "        schedule = fastf1.get_event_schedule(year)\n",
    "        \n",
    "        race_events = schedule.loc[schedule['EventFormat'].isin(['conventional', 'sprint'])]\n",
    "        \n",
    "        if race_events.empty:\n",
    "            year_loading_status[year]['status'] = 'No Race Events'\n",
    "            year_loading_status[year]['errors'].append(f\"No 'conventional' or 'sprint' race events found for {year}. Skipping season.\")\n",
    "            # If the current year (2025) has no events, we can stop\n",
    "            if year == end_year:\n",
    "                print(f\"No events found for the current season {year}. Stopping data collection.\")\n",
    "                break # Exit the year loop\n",
    "            continue\n",
    "\n",
    "        for round_num in race_events['RoundNumber']:\n",
    "            error_in_current_round = False # Flag to check for error in this specific round\n",
    "            \n",
    "            # Pause before attempting to load each round's data\n",
    "            time.sleep(1.5) # Increased pause to 1.5 seconds for more reliability\n",
    "            \n",
    "            try:\n",
    "                # --- Load Race Session Results ---\n",
    "                race_session = fastf1.get_session(year, round_num, 'R')\n",
    "                # Load with default parameters, FastF1 automatically caches here.\n",
    "                # The warnings about telemetry, laps etc. are fine as they are not needed.\n",
    "                race_session.load(telemetry=False, laps=False, weather=False) \n",
    "\n",
    "                if race_session.results.empty:\n",
    "                    year_loading_status[year]['errors'].append(f\"WARNING: No results for Race {year} R{round_num}. Data unavailable.\")\n",
    "                    error_in_current_round = True\n",
    "                else:\n",
    "                    results_df = race_session.results.copy()\n",
    "                    results_df['Season'] = year\n",
    "                    results_df['Round'] = round_num\n",
    "                    results_df['EventName'] = race_session.event['EventName']\n",
    "                    results_df['SessionType'] = 'Race'\n",
    "                    all_race_results.append(results_df)\n",
    "                    year_loading_status[year]['races_loaded'] += 1\n",
    "\n",
    "                # Small pause between race and qualifying session loads for the same round\n",
    "                time.sleep(0.75) # Pause for 0.75 seconds\n",
    "\n",
    "                # --- Optionally, load Qualifying Session Results ---\n",
    "                try:\n",
    "                    quali_session = fastf1.get_session(year, round_num, 'Q')\n",
    "                    quali_session.load(telemetry=False, laps=False, weather=False)\n",
    "\n",
    "                    if quali_session.results.empty:\n",
    "                        year_loading_status[year]['errors'].append(f\"WARNING: No results for Qualifying {year} R{round_num}. Data unavailable.\")\n",
    "                        error_in_current_round = True\n",
    "                    else:\n",
    "                        quali_results_df = quali_session.results.copy()\n",
    "                        quali_results_df['Season'] = year\n",
    "                        quali_results_df['Round'] = round_num\n",
    "                        quali_results_df['EventName'] = quali_session.event['EventName']\n",
    "                        quali_results_df['SessionType'] = 'Qualifying'\n",
    "                        all_qualifying_results.append(quali_results_df)\n",
    "                        year_loading_status[year]['qual_loaded'] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    year_loading_status[year]['errors'].append(f\"Error loading Qualifying for {year} R{round_num}: {e}.\")\n",
    "                    error_in_current_round = True\n",
    "\n",
    "            except Exception as e:\n",
    "                year_loading_status[year]['errors'].append(f\"CRITICAL ERROR loading session {year} R{round_num}: {e}.\")\n",
    "                error_in_current_round = True\n",
    "            \n",
    "            # --- Stop condition for the current ongoing season (2025) ---\n",
    "            if year == end_year and error_in_current_round:\n",
    "                print(f\"Detected a data loading issue for {year} Round {round_num}. Stopping further data collection for the current season.\")\n",
    "                break # Exit the inner loop (round_num)\n",
    "        \n",
    "        # After attempting all rounds for the year, or if we broke early\n",
    "        # Check if we should stop the entire process\n",
    "        if year == end_year and year_loading_status[year]['errors']:\n",
    "            # This covers cases where all rounds were attempted, but errors occurred,\n",
    "            # or if we broke early from the inner loop due to an error in 2025.\n",
    "            print(f\"Errors found in the current season ({year}). Stopping overall data collection.\")\n",
    "            break # Exit the outer loop (year)\n",
    "\n",
    "        # Determine overall status for the year\n",
    "        if not year_loading_status[year]['errors']:\n",
    "            year_loading_status[year]['status'] = 'SUCCESS'\n",
    "        elif year_loading_status[year]['races_loaded'] > 0 or year_loading_status[year]['qual_loaded'] > 0:\n",
    "            year_loading_status[year]['status'] = 'PARTIAL SUCCESS (with errors)'\n",
    "        else:\n",
    "            year_loading_status[year]['status'] = 'FAILED'\n",
    "\n",
    "    except Exception as e:\n",
    "        year_loading_status[year]['status'] = 'SCHEDULE ERROR'\n",
    "        year_loading_status[year]['errors'].append(f\"Error retrieving schedule for {year}: {e}.\")\n",
    "        # If schedule fails for the current ongoing year, stop.\n",
    "        if year == end_year:\n",
    "            print(f\"Schedule loading failed for the current season ({year}). Stopping overall data collection.\")\n",
    "            break # Exit the year loop\n",
    "\n",
    "# --- Print Summary of Data Loading ---\n",
    "print(\"\\n--- Data Loading Summary by Year ---\")\n",
    "for year, status_info in year_loading_status.items():\n",
    "    print(f\"Season {year}: {status_info['status']}\")\n",
    "    if status_info['races_loaded'] > 0:\n",
    "        print(f\"  Races Loaded: {status_info['races_loaded']}\")\n",
    "    if status_info['qual_loaded'] > 0:\n",
    "        print(f\"  Qualifying Sessions Loaded: {status_info['qual_loaded']}\")\n",
    "    if status_info['errors']:\n",
    "        print(f\"  Issues Encountered ({len(status_info['errors'])}):\")\n",
    "        for error_msg in status_info['errors']:\n",
    "            print(f\"    - {error_msg}\")\n",
    "    print(\"-\" * 30) # Separator\n",
    "\n",
    "# --- 3. Concatenate All DataFrames ---\n",
    "print(\"\\n--- Concatenating collected data ---\")\n",
    "if all_race_results:\n",
    "    full_race_results_df = pd.concat(all_race_results, ignore_index=True)\n",
    "    print(f\"Successfully collected {len(full_race_results_df)} race results entries.\")\n",
    "else:\n",
    "    print(\"No race results data collected into a DataFrame.\")\n",
    "\n",
    "if all_qualifying_results:\n",
    "    full_quali_results_df = pd.concat(all_qualifying_results, ignore_index=True)\n",
    "    print(f\"Successfully collected {len(full_quali_results_df)} qualifying results entries.\")\n",
    "else:\n",
    "    print(\"No qualifying results data collected into a DataFrame.\")\n",
    "\n",
    "print(\"\\n--- Data collection process finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We now handle any missing values in the dataframe, ensuring appropriate format for training.\n",
    "The code cell below reveals the columns in `full_race_results_df`, from which we will remove the ones unnecessary in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_race_results_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns `'DriverNumber', 'BroadcastName', 'DriverId', 'TeamColor', 'TeamId', 'FirstName', 'LastName', 'FullName', 'HeadshotUrl', 'CountryCode', 'ClassifiedPosition', 'Q1', 'Q2', 'Q3', 'Time', 'Status', 'Points', 'Laps', 'SessionType'` are dropped from the original DataFrame, and the resulting DataFrame is stored in `new_df`.\n",
    "\n",
    "As the model will be made to predict the raw `Position` of each driver, `ClassifiedPosition` is removed since race retirement is extremely situational and `Position` will instead be used as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data_for_merge = full_race_results_df.drop(['DriverNumber', 'BroadcastName', 'DriverId', 'TeamColor', 'TeamId', 'FirstName', 'LastName', 'FullName', 'HeadshotUrl', 'CountryCode', 'ClassifiedPosition', 'Q1', 'Q2', 'Q3', 'Time', 'Status', 'Points', 'Laps', 'SessionType'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Q1', 'Q2', and 'Q3'` are `timedelta` objects and are converted into seconds and stored respectively in the new columns `'Q1_s', 'Q2_s', and 'Q3_s'`. This will allow for its meaningful use when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns to hold values of 'Q1', 'Q2, and 'Q3' in seconds.\n",
    "full_quali_results_df['Q1_s'] = full_quali_results_df['Q1'].dt.total_seconds().fillna(9999.0)\n",
    "full_quali_results_df['Q2_s'] = full_quali_results_df['Q2'].dt.total_seconds().fillna(9999.0)\n",
    "full_quali_results_df['Q3_s'] = full_quali_results_df['Q3'].dt.total_seconds().fillna(9999.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since certain qualifying data are useful for training the model, we merge a subset of the 2 dataframes `full_race_results_df` and `full_quali_results_df`, keeping only the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quali_data_for_merge = full_quali_results_df[['Abbreviation', 'Season', 'Round', 'EventName', 'Q1_s', 'Q2_s', 'Q3_s']].copy()\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    race_data_for_merge,\n",
    "    quali_data_for_merge,\n",
    "    on=['Abbreviation', 'Season', 'Round', 'EventName'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Organising columns for readability and easier management\n",
    "merged_df = merged_df[[\n",
    "    'Season', \n",
    "    'Round',\n",
    "    'EventName',\n",
    "    'Abbreviation', # Driver identifier eg. 'VER', 'PER', 'ALO'\n",
    "    'TeamName',\n",
    "    'GridPosition', # Qualifying positions, also positions in which they start on during race\n",
    "    'Q1_s', # Q1 time in seconds\n",
    "    'Q2_s',\n",
    "    'Q3_s',\n",
    "    'Position' # The target variable for race prediction\n",
    "]]\n",
    "\n",
    "merged_df.head(20) # DataFrame snippet of race 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the 2023 Singapore Grand Prix, driver Lance Stroll was withdrawn from the Sunday race following a heavy crash during Saturday's qualifying session. \n",
    "\n",
    "As a result, we see an empty value for Stroll's `'GridPosition'` and `'Position'`for the event.\n",
    "\n",
    "As there are instances (rows 197, 241, 278, 633) where drivers starting from the pitlanes has resulted in `'GridPosition'` and `'Position'` holding the value of 0.0, we deduce that this is an anomaly on the side of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result should be 1, referring to this singular anomaly\n",
    "print(\"Number of rows with missing values for 'GridPosition' before cleaning:\", merged_df['GridPosition'].isna().sum())\n",
    "\n",
    "# A simple fix would be to replace all missing values with 0.0\n",
    "merged_df.loc[merged_df['GridPosition'].isna(), 'GridPosition'] = 0.0\n",
    "merged_df.loc[merged_df['Position'].isna(), 'Position'] = 0.0\n",
    "merged_df.loc[merged_df['Q1_s'].isna(), 'Q1_s'] = 0.0\n",
    "merged_df.loc[merged_df['Q2_s'].isna(), 'Q2_s'] = 0.0\n",
    "merged_df.loc[merged_df['Q3_s'].isna(), 'Q3_s'] = 0.0\n",
    "\n",
    "# Result after cleaning\n",
    "print(\"Number of rows with missing values for 'GridPosition' after cleaning:\", merged_df['GridPosition'].isna().sum())\n",
    "\n",
    "#merged_df.to_csv(os.path.join(os.getcwd(), 'merged_df.csv'), index=False) # This optional line creates a .csv file for better visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv\n",
    "\n",
    "Given the limitations of the fastf1 API, race datas for certain years are at times omitted, causing caching issues and missing data from the resulting `merged_df`. \n",
    "\n",
    "As a solution, in `2015_to_2025_df.csv` are merged data acquired separately. This (more) completed data will be loaded into `merged_df` in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\CLL\\OneDrive\\Documents\\GitHub\\F1-Predictor\\notebooks\n",
      "Successfully loaded data from '2015_to_2025_df.csv' into a DataFrame.\n",
      "\n",
      "DataFrame info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4458 entries, 0 to 4457\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Season        4458 non-null   int64  \n",
      " 1   Round         4458 non-null   int64  \n",
      " 2   EventName     4458 non-null   object \n",
      " 3   Abbreviation  4458 non-null   object \n",
      " 4   TeamName      4458 non-null   object \n",
      " 5   GridPosition  4458 non-null   float64\n",
      " 6   Q1_s          4458 non-null   float64\n",
      " 7   Q2_s          4458 non-null   float64\n",
      " 8   Q3_s          4458 non-null   float64\n",
      " 9   Position      4458 non-null   float64\n",
      "dtypes: float64(5), int64(2), object(3)\n",
      "memory usage: 348.4+ KB\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "csv_file_path = '2015_to_2025_df.csv'\n",
    "\n",
    "try:\n",
    "    merged_df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Successfully loaded data from '{csv_file_path}' into a DataFrame.\")\n",
    "    print(f\"\\nDataFrame info:\\n\")\n",
    "    merged_df.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    print(f\"Please ensure '{csv_file_path}' is in your current working directory: {current_directory}\")\n",
    "    print(\"If it's in a different location relative to this notebook, adjust the 'csv_file_path'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "Now that we have the prepared `merged_df`, we have to convert categorical (non-numerical) data into a numerical format that machine learning algorithms can understand and process effectively. \n",
    "\n",
    "Let us look at the `'TeamName'` column. Without one-hot encoding, if we were to simply assign each team values eg. Red Bull = 1, Ferrari = 2, Mercedes = 3, a machine learning model would interpret these numbers as having an inherent order or magnitude. It might think that Mercedes (3) is \"better\" or \"more important\" or \"further away\" from Red Bull (1) than Ferrari (2) is. This is clearly false for nominal categories like team names, where there's no inherent numerical relationship. This false ordinality can mislead the model and lead to incorrect predictions or reduced performance.\n",
    "\n",
    "With one-hot encoding, we create new binary (0 or 1) columns for every team that exists. If in row 1 was Max Verstappen from team Red bull Racing, we would now have a value of 1 in Team_RedBull and a 0 in the columns of every other team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original DataFrame shape: (4458, 10)\n",
      "Encoded DataFrame shape: (4458, 119)\n",
      "Fitted OneHotEncoder saved successfully to: ..\\models\\one_hot_encoder.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define categorical columns for one-hot encoding\n",
    "categorical_cols_to_encode = ['Abbreviation', 'TeamName', 'EventName']\n",
    "\n",
    "print(f\"\\nOriginal DataFrame shape: {merged_df.shape}\")\n",
    "\n",
    "# --- Initialize and Fit One-Hot Encoder ---\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the categorical columns of your training data\n",
    "ohe.fit(merged_df[categorical_cols_to_encode])\n",
    "\n",
    "# --- Transform the DataFrame ---\n",
    "# Apply the fitted encoder to transform the categorical columns\n",
    "encoded_features = ohe.transform(merged_df[categorical_cols_to_encode])\n",
    "\n",
    "# Create a DataFrame from the encoded features with proper column names\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_features.astype(int), \n",
    "    columns=ohe.get_feature_names_out(categorical_cols_to_encode),\n",
    "    index=merged_df.index\n",
    ")\n",
    "\n",
    "# Drop the original categorical columns from merged_df\n",
    "numerical_and_other_features_df = merged_df.drop(columns=categorical_cols_to_encode)\n",
    "\n",
    "# Concatenate the original numerical/other features with the new encoded features\n",
    "merged_df_encoded = pd.concat([numerical_and_other_features_df, encoded_df], axis=1)\n",
    "\n",
    "# Note the increase in number of columns after encoding\n",
    "print(f\"Encoded DataFrame shape: {merged_df_encoded.shape}\")\n",
    "\n",
    "\n",
    "# --- IMPORTANT: Save the fitted OneHotEncoder ---\n",
    "# This is crucial for preprocessing.py to work correctly for new data.\n",
    "models_dir = os.path.join('..', 'models')\n",
    "os.makedirs(models_dir, exist_ok=True) # Ensure the 'models' directory exists\n",
    "\n",
    "joblib.dump(ohe, os.path.join(models_dir, 'one_hot_encoder.joblib'))\n",
    "print(f\"Fitted OneHotEncoder saved successfully to: {os.path.join(models_dir, 'one_hot_encoder.joblib')}\")\n",
    "\n",
    "# You will also need to save your scaler and the final training feature names, as discussed previously.\n",
    "joblib.dump(merged_df_encoded.columns.tolist(), os.path.join(models_dir, 'training_feature_names.joblib'))\n",
    "\n",
    "#merged_df_encoded.to_csv(os.path.join(os.getcwd(), 'merged_df_encoded.csv'), index=False) # This optional line creates a .csv file for better visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'Round', 'GridPosition', 'Q1_s', 'Q2_s', 'Q3_s', 'Position',\n",
       "       'Abbreviation_ALB', 'Abbreviation_ALO', 'Abbreviation_BEA',\n",
       "       ...\n",
       "       'EventName_Russian Grand Prix', 'EventName_Sakhir Grand Prix',\n",
       "       'EventName_Saudi Arabian Grand Prix', 'EventName_Singapore Grand Prix',\n",
       "       'EventName_Spanish Grand Prix', 'EventName_Styrian Grand Prix',\n",
       "       'EventName_São Paulo Grand Prix', 'EventName_Turkish Grand Prix',\n",
       "       'EventName_Tuscan Grand Prix', 'EventName_United States Grand Prix'],\n",
       "      dtype='object', length=115)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Splitting data into features (X) and the target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X shape: (4458, 118)\n",
      "y shape: (4458,)\n"
     ]
    }
   ],
   "source": [
    "# Define the target variable\n",
    "y = merged_df_encoded['Position']\n",
    "\n",
    "# Define the features (X) by dropping the target and any other non-feature columns\n",
    "columns_to_exclude_from_X = ['Position'] # Only exclude the target variable\n",
    "\n",
    "X = merged_df_encoded.drop(columns=columns_to_exclude_from_X, axis=1)\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set shape (X_train, y_train): (3566, 118), (3566,)\n",
      "Test set shape (X_test, y_test): (892, 118), (892,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data with 80% for training and 20% for testing\n",
    "# random_state ensures reproducibility of your split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain set shape (X_train, y_train): {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set shape (X_test, y_test): {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose __Candidate Models__ and define __Hyperparameter Grids__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# Define the scoring metric for optimization\n",
    "# For MAE, lower is better, so we use 'neg_mean_absolute_error' for GridSearchCV\n",
    "# which maximizes the score.\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# --- Model 1: RandomForestRegressor ---\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': [0.6, 0.8, 1.0],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# --- Model 2: GradientBoostingRegressor ---\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 700],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "}\n",
    "\n",
    "# You could add more models here, e.g.,\n",
    "# from xgboost import XGBRegressor\n",
    "# xgb_model = XGBRegressor(random_state=42)\n",
    "# xgb_param_grid = { ... }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform __Hyperparameter Tuning__ with Cross-Validation (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting GridSearchCV for RandomForestRegressor ---\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Best parameters for RandomForestRegressor: {'max_features': 0.6, 'min_samples_leaf': 4, 'n_estimators': 300}\n",
      "Best cross-validated MAE for RandomForestRegressor: 3.4325623870899755\n",
      "\n",
      "--- Starting GridSearchCV for GradientBoostingRegressor ---\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Best parameters for GradientBoostingRegressor: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 300}\n",
      "Best cross-validated MAE for GradientBoostingRegressor: 3.3784689563535664\n"
     ]
    }
   ],
   "source": [
    "# --- GridSearchCV for RandomForestRegressor ---\n",
    "print(\"\\n--- Starting GridSearchCV for RandomForestRegressor ---\")\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring=mae_scorer,\n",
    "    cv=5,            # 5-fold cross-validation\n",
    "    n_jobs=-1,       # Use all available CPU cores\n",
    "    verbose=1        # Show progress\n",
    ")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters for RandomForestRegressor:\", rf_grid_search.best_params_)\n",
    "print(\"Best cross-validated MAE for RandomForestRegressor:\", -rf_grid_search.best_score_)\n",
    "\n",
    "\n",
    "# --- GridSearchCV for GradientBoostingRegressor ---\n",
    "print(\"\\n--- Starting GridSearchCV for GradientBoostingRegressor ---\")\n",
    "gb_grid_search = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=gb_param_grid,\n",
    "    scoring=mae_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters for GradientBoostingRegressor:\", gb_grid_search.best_params_)\n",
    "print(\"Best cross-validated MAE for GradientBoostingRegressor:\", -gb_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Models on the Test Set\n",
    "\n",
    "| Metric | What it measures (broadly)                                | Smaller or Larger is Better? |\n",
    "|--------|-----------------------------------------------------------|------------------------------|\n",
    "| MAE    | Average error magnitude                                   | Smaller                      |\n",
    "| RMSE   | Average magnitude of errors, penalizing large errors more | Smaller                      |\n",
    "| R²     | Proportion of variance explained by the model             | Larger                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 5: Evaluate the Best Models on the Test Set ---\n",
      "\n",
      "--- RandomForestRegressor Evaluation ---\n",
      "RandomForestRegressor MAE on Test Set: 3.5074\n",
      "RandomForestRegressor R2 on Test Set: 0.3836\n",
      "RandomForestRegressor RMSE on Test Set: 4.5365\n",
      "\n",
      "--- GradientBoostingRegressor Evaluation ---\n",
      "GradientBoostingRegressor MAE on Test Set: 3.3974\n",
      "GradientBoostingRegressor R2 on Test Set: 0.4084\n",
      "GradientBoostingRegressor RMSE on Test Set: 4.4443\n",
      "\n",
      "--- Model Comparison ---\n",
      "GradientBoostingRegressor performed better in terms of MAE.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "print(\"--- Step 5: Evaluate the Best Models on the Test Set ---\")\n",
    "\n",
    "# Retrieve the best models from GridSearchCV\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "best_gb_model = gb_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = best_rf_model.predict(X_test)\n",
    "gb_predictions = best_gb_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- RandomForestRegressor Evaluation ---\")\n",
    "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
    "rf_r2 = r2_score(y_test, rf_predictions)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "\n",
    "print(f\"RandomForestRegressor MAE on Test Set: {rf_mae:.4f}\")\n",
    "print(f\"RandomForestRegressor R2 on Test Set: {rf_r2:.4f}\")\n",
    "print(f\"RandomForestRegressor RMSE on Test Set: {rf_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n--- GradientBoostingRegressor Evaluation ---\")\n",
    "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
    "gb_r2 = r2_score(y_test, gb_predictions)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
    "\n",
    "print(f\"GradientBoostingRegressor MAE on Test Set: {gb_mae:.4f}\")\n",
    "print(f\"GradientBoostingRegressor R2 on Test Set: {gb_r2:.4f}\")\n",
    "print(f\"GradientBoostingRegressor RMSE on Test Set: {gb_rmse:.4f}\")\n",
    "\n",
    "# You can also compare them:\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "if rf_mae < gb_mae:\n",
    "    print(\"RandomForestRegressor performed better in terms of MAE.\")\n",
    "elif gb_mae < rf_mae:\n",
    "    print(\"GradientBoostingRegressor performed better in terms of MAE.\")\n",
    "else:\n",
    "    print(\"Both models have similar MAE.\")\n",
    "\n",
    "# Optional: Store evaluation metrics for later analysis if needed\n",
    "evaluation_results = {\n",
    "    'RandomForestRegressor': {'MAE': rf_mae, 'R2': rf_r2, 'RMSE': rf_rmse},\n",
    "    'GradientBoostingRegressor': {'MAE': gb_mae, 'R2': gb_r2, 'RMSE': gb_rmse}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Final Model and Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Selection ---\n",
      "\n",
      "Selected final model: GradientBoosting\n",
      "Best parameters for selected model:\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 300, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Model saved successfully to: ..\\models\\V2_GradientBoosting_F1_Race_Predictor_model.joblib\n",
      "\n",
      "--- Verifying Model Loading (Optional) ---\n",
      "Model loaded successfully: GradientBoostingRegressor(learning_rate=0.05, n_estimators=300, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"--- Model Selection ---\")\n",
    "\n",
    "# Based on evaluation results, GradientBoostingRegressor is the chosen model.\n",
    "# Assuming gb_grid_search is defined and available from previous cells in the notebook\n",
    "best_model_to_save = gb_grid_search.best_estimator_ # Access the best estimator from GridSearchCV\n",
    "model_name = \"GradientBoosting\"\n",
    "\n",
    "print(f\"\\nSelected final model: {model_name}\")\n",
    "print(\"Best parameters for selected model:\")\n",
    "print(best_model_to_save.get_params())\n",
    "\n",
    "\n",
    "# Model Persistence (Saving the Model)\n",
    "# Define the relative path to the 'models' directory\n",
    "# This assumes the notebook is run from within the 'notebooks' directory\n",
    "# and 'models' is a sibling directory to 'notebooks'\n",
    "model_dir = os.path.join('..', 'models')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Construct the full path for the model file\n",
    "model_filename = os.path.join(model_dir, f'{model_name}_F1_Race_Predictor_model.joblib')\n",
    "\n",
    "joblib.dump(best_model_to_save, model_filename, compress=3)\n",
    "\n",
    "print(f\"\\nModel saved successfully to: {model_filename}\")\n",
    "\n",
    "# Loading the Model to Verify\n",
    "print(\"\\n--- Verifying Model Loading (Optional) ---\")\n",
    "loaded_model = joblib.load(model_filename)\n",
    "print(f\"Model loaded successfully: {loaded_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
